---
layout: page
title: Reading list
---
 
<div class="message">
  Apart from the papers listed below, presenter may also discuss few other relevent papers. 
  Please contact Hardik to add/remove the paper in this list. 
</div>

1. **Polynomial networks, Factorization machines**
    * [On the Computational Efficiency of Training Neural Networks](http://papers.nips.cc/paper/5267-on-the-computational-efficiency-of-training-neural-networks.pdf), Livni et al., NIPS 2014 [(Details)](https://virajshah018.github.io//2017/06/09/week1/)
    * [Polynomial networks and factorization machines](https://arxiv.org/pdf/1607.08810.pdf), Blondel et al., ICML 2016[(Details)](https://virajshah018.github.io//2017/06/09/week1/)
    * [Multi-Output factorization machines](https://arxiv.org/abs/1705.07603), Blondel et al, preprint 2017
2. **Invertibility of neural nets**
    * [Learning RELUs via gradient descent](https://arxiv.org/abs/1705.04591), Soltanolkotabi, 2017. [(Details)](https://virajshah018.github.io//2017/06/12/week2/)
    * [Towards understanding invertibility of CNNs](https://arxiv.org/abs/1705.08664), Gilbert et al, 2017.[(Details)](https://virajshah018.github.io//2017/06/12/week2/)
    * [Identity matters in deep learning](https://arxiv.org/pdf/1611.04231.pdf), Hardt and Ma, 2017.
3. **Recurrent models and sparse coding**
    * [Recurrent inference machines](http://www.ics.uci.edu/~welling/publications/papers/Submitted2016-RIM.pdf), Welling et al.
    * [Understanding Sparse Coding via Matrix Factorization](https://arxiv.org/pdf/1609.00285.pdf), Bruna 2017
    * [Convolutional NNs and convolutional sparse coding](https://arxiv.org/pdf/1607.08194.pdf), Elad et al.
    * [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf), Andrychowicz et al. 2016 
4. **Statistical aspects of neural net learning**
    * [Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of Dimensionality: a Review](https://arxiv.org/pdf/1611.00740.pdf), Poggio et al., 2016.
    * [L1-regularized Neural Networks are Improperly Learnable in Polynomial Time](http://proceedings.mlr.press/v54/zhang17a/zhang17a.pdf), Wainwright et al., 2017.
    * [Understanding neural networks requires rethinking generalization](https://arxiv.org/abs/1611.03530), Zhang et al, ICLR 2017.
5. **Generative models**
    * [Compressed sensing using generative models](https://arxiv.org/pdf/1703.03208.pdf), Price et al., ICML 2017.
    * [Generation and Equilibrium in GANs](https://arxiv.org/pdf/1703.00573.pdf), Arora et al, 2017
    * [Wasserstein GANs](https://arxiv.org/pdf/1701.07875.pdf), Arjovsky, 2017.
    * [Stabilizing GANs via random projections](https://arxiv.org/pdf/1705.07831.pdf), Neyshabur et al, 2017
    * [Global guarantees for Enforcing Deep Generative Priors](https://arxiv.org/pdf/1705.07576.pdf), Hand et al, 2017.
    * [Towards Understanding the Dynamics of Generative Adversarial Networks](https://arxiv.org/pdf/1706.09884.pdf), Li et al, 2017.



